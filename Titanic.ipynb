{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzOLpUehImFu0DLVYBA2ey",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jungeun919/Pytorch_study/blob/main/Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP-s3nGkpqXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e62477-0012-4eba-f33c-237f067e58c8"
      },
      "source": [
        "# Dataset & Library Loading\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from collections import Counter\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoFnsnfnp7QO"
      },
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Titanic/titanic/train.csv')\r\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Titanic/titanic/test.csv')\r\n",
        "df_sub = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Titanic/titanic/gender_submission1.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpsnNHyXqPEj"
      },
      "source": [
        "# Make Dataset\r\n",
        "\r\n",
        "df_train.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True) # inplace의 경우 drop한 후의 데이터프레임으로 기존 데이터프레임을 대체하겠다는 뜻\r\n",
        "df_test.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\r\n",
        "\r\n",
        "sex = pd.get_dummies(df_train['Sex'], drop_first=True) # 가변수의 첫번째 변수를 자동으로 삭제를 해주며, 가변수 함정을 피할 수 있게 해줌\r\n",
        "embark = pd.get_dummies(df_train['Embarked'], drop_first=True)\r\n",
        "df_train = pd.concat([df_train, sex, embark], axis=1) # DataFrame 결합\r\n",
        "\r\n",
        "df_train.drop(['Sex', 'Embarked'], axis=1, inplace=True)\r\n",
        "\r\n",
        "sex = pd.get_dummies(df_test['Sex'], drop_first=True)\r\n",
        "embark = pd.get_dummies(df_test['Embarked'], drop_first=True)\r\n",
        "df_test = pd.concat([df_test, sex, embark], axis=1)\r\n",
        "\r\n",
        "df_test.drop(['Sex', 'Embarked'], axis=1, inplace=True)\r\n",
        "\r\n",
        "df_train.fillna(df_train.mean(), inplace=True) # 결측값 평균값으로 채우기\r\n",
        "df_test.fillna(df_test.mean(), inplace=True)\r\n",
        "\r\n",
        "Scaler1 = StandardScaler() # 평균을 제거하고 데이터를 단위 분산으로 조정\r\n",
        "Scaler2 = StandardScaler() # 그러나 이상치가 있다면 평균과 표준편차에 영향을 미쳐 변환된 데이터의 확산은 매우 달라짐\r\n",
        "\r\n",
        "train_columns = df_train.columns\r\n",
        "test_columns = df_test.columns\r\n",
        "\r\n",
        "df_train = pd.DataFrame(Scaler1.fit_transform(df_train))\r\n",
        "df_test = pd.DataFrame(Scaler2.fit_transform(df_test))\r\n",
        "\r\n",
        "df_train.columns = train_columns\r\n",
        "df_test.columns = test_columns\r\n",
        "\r\n",
        "\r\n",
        "# iloc: 행번호로 선택하는 방법\r\n",
        "# loc: label이나 조건표현으로 선택하는 방법\r\n",
        "feature = df_train.iloc[:, 2:].columns.tolist() # tolist(): 같은 위치에 있는 데이터끼리 묶어준다 ex) 'value'=[1, 2, 3], 'test=['a', 'b', 'c']\r\n",
        "target = df_train.loc[:, 'Survived'].name\r\n",
        "\r\n",
        "X_train = df_train.iloc[:, 2:].values\r\n",
        "y_train = df_train.loc[:, 'Survived'].values"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mYJI3P-sYtg"
      },
      "source": [
        "# Pytorch\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from torch.autograd import Variable # autograd: 자동 미분화(역전파)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpPsGVzLsnB-",
        "outputId": "ab240ce7-187c-4942-d5bf-b5292f5e32a1"
      },
      "source": [
        "# Logistic Regression Model\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.fc1 = nn.Linear(8, 512)\r\n",
        "        self.fc2 = nn.Linear(512, 512)\r\n",
        "        self.fc3 = nn.Linear(512, 2)\r\n",
        "        self.dropout = nn.Dropout(0.2)\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = self.dropout(x)\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        x = self.dropout(x)\r\n",
        "        x = self.fc3(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "model = Net()\r\n",
        "print(model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CJbn5pQtab3"
      },
      "source": [
        "# Optimizer (확률적 경사하강법 SGD)\r\n",
        "# Loss Function\r\n",
        "\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\r\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hi9YPz5t7vR",
        "outputId": "d66eb007-bab8-4bc3-fb83-1c41ad460d9c"
      },
      "source": [
        "# Training\r\n",
        "\r\n",
        "batch_size = 64\r\n",
        "n_epochs = 500\r\n",
        "batch_no = len(X_train) // batch_size\r\n",
        "\r\n",
        "train_loss = 0\r\n",
        "train_loss_min = np.Inf\r\n",
        "for epoch in range(n_epochs):\r\n",
        "    for i in range(batch_no):\r\n",
        "        start = i * batch_size\r\n",
        "        end = start + batch_size\r\n",
        "        x_var = Variable(torch.FloatTensor(X_train[start:end]))\r\n",
        "        y_var = Variable(torch.LongTensor(y_train[start:end]))\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        output = model(x_var)\r\n",
        "        loss = criterion(output, y_var)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        values, labels = torch.max(output, 1)\r\n",
        "        num_right = np.sum(labels.data.numpy() == y_train[start:end])\r\n",
        "        train_loss += loss.item() * batch_size\r\n",
        "\r\n",
        "    train_loss = train_loss / len(X_train)\r\n",
        "    if train_loss <= train_loss_min:\r\n",
        "        print(\"Validation loss decreased ({:6f} ===> {:6f}). Saving  the model...\"\\\r\n",
        "              .format(train_loss_min, train_loss))\r\n",
        "        torch.save(model.state_dict(), \"model.pt\")\r\n",
        "        train_loss_min = train_loss\r\n",
        "\r\n",
        "    if epoch % 200 == 0:\r\n",
        "        print('')\r\n",
        "        print(\"Epoch: {} \\tTrain Loss: {} \\tTrain Accuracy: {}\"\\\r\n",
        "              .format(epoch+1, train_loss, num_right / len(y_train[start:end])))\r\n",
        "        print('Training Ended!')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation loss decreased (   inf ===> 0.666620). Saving  the model...\n",
            "\n",
            "Epoch: 1 \tTrain Loss: 0.6666198370833991 \tTrain Accuracy: 0.0\n",
            "Training Ended!\n",
            "Validation loss decreased (0.666620 ===> 0.664644). Saving  the model...\n",
            "Validation loss decreased (0.664644 ===> 0.664220). Saving  the model...\n",
            "Validation loss decreased (0.664220 ===> 0.658981). Saving  the model...\n",
            "Validation loss decreased (0.658981 ===> 0.655179). Saving  the model...\n",
            "Validation loss decreased (0.655179 ===> 0.654461). Saving  the model...\n",
            "Validation loss decreased (0.654461 ===> 0.654140). Saving  the model...\n",
            "Validation loss decreased (0.654140 ===> 0.654075). Saving  the model...\n",
            "Validation loss decreased (0.654075 ===> 0.649203). Saving  the model...\n",
            "Validation loss decreased (0.649203 ===> 0.646196). Saving  the model...\n",
            "Validation loss decreased (0.646196 ===> 0.646019). Saving  the model...\n",
            "Validation loss decreased (0.646019 ===> 0.645368). Saving  the model...\n",
            "Validation loss decreased (0.645368 ===> 0.643947). Saving  the model...\n",
            "Validation loss decreased (0.643947 ===> 0.642205). Saving  the model...\n",
            "Validation loss decreased (0.642205 ===> 0.639671). Saving  the model...\n",
            "Validation loss decreased (0.639671 ===> 0.639476). Saving  the model...\n",
            "Validation loss decreased (0.639476 ===> 0.638163). Saving  the model...\n",
            "Validation loss decreased (0.638163 ===> 0.636671). Saving  the model...\n",
            "Validation loss decreased (0.636671 ===> 0.635752). Saving  the model...\n",
            "Validation loss decreased (0.635752 ===> 0.631531). Saving  the model...\n",
            "Validation loss decreased (0.631531 ===> 0.630402). Saving  the model...\n",
            "Validation loss decreased (0.630402 ===> 0.629442). Saving  the model...\n",
            "Validation loss decreased (0.629442 ===> 0.628251). Saving  the model...\n",
            "Validation loss decreased (0.628251 ===> 0.627883). Saving  the model...\n",
            "Validation loss decreased (0.627883 ===> 0.627860). Saving  the model...\n",
            "Validation loss decreased (0.627860 ===> 0.625103). Saving  the model...\n",
            "Validation loss decreased (0.625103 ===> 0.623460). Saving  the model...\n",
            "Validation loss decreased (0.623460 ===> 0.622024). Saving  the model...\n",
            "Validation loss decreased (0.622024 ===> 0.621121). Saving  the model...\n",
            "Validation loss decreased (0.621121 ===> 0.618355). Saving  the model...\n",
            "Validation loss decreased (0.618355 ===> 0.617853). Saving  the model...\n",
            "Validation loss decreased (0.617853 ===> 0.615747). Saving  the model...\n",
            "Validation loss decreased (0.615747 ===> 0.615692). Saving  the model...\n",
            "Validation loss decreased (0.615692 ===> 0.615500). Saving  the model...\n",
            "Validation loss decreased (0.615500 ===> 0.614136). Saving  the model...\n",
            "Validation loss decreased (0.614136 ===> 0.611217). Saving  the model...\n",
            "Validation loss decreased (0.611217 ===> 0.609685). Saving  the model...\n",
            "Validation loss decreased (0.609685 ===> 0.609592). Saving  the model...\n",
            "Validation loss decreased (0.609592 ===> 0.606925). Saving  the model...\n",
            "Validation loss decreased (0.606925 ===> 0.606701). Saving  the model...\n",
            "Validation loss decreased (0.606701 ===> 0.605587). Saving  the model...\n",
            "Validation loss decreased (0.605587 ===> 0.603958). Saving  the model...\n",
            "Validation loss decreased (0.603958 ===> 0.602553). Saving  the model...\n",
            "Validation loss decreased (0.602553 ===> 0.602401). Saving  the model...\n",
            "Validation loss decreased (0.602401 ===> 0.601532). Saving  the model...\n",
            "Validation loss decreased (0.601532 ===> 0.601167). Saving  the model...\n",
            "Validation loss decreased (0.601167 ===> 0.600762). Saving  the model...\n",
            "Validation loss decreased (0.600762 ===> 0.600554). Saving  the model...\n",
            "Validation loss decreased (0.600554 ===> 0.599730). Saving  the model...\n",
            "Validation loss decreased (0.599730 ===> 0.598682). Saving  the model...\n",
            "Validation loss decreased (0.598682 ===> 0.597723). Saving  the model...\n",
            "Validation loss decreased (0.597723 ===> 0.596203). Saving  the model...\n",
            "Validation loss decreased (0.596203 ===> 0.591039). Saving  the model...\n",
            "Validation loss decreased (0.591039 ===> 0.589295). Saving  the model...\n",
            "Validation loss decreased (0.589295 ===> 0.588203). Saving  the model...\n",
            "Validation loss decreased (0.588203 ===> 0.586893). Saving  the model...\n",
            "Validation loss decreased (0.586893 ===> 0.585881). Saving  the model...\n",
            "Validation loss decreased (0.585881 ===> 0.584899). Saving  the model...\n",
            "Validation loss decreased (0.584899 ===> 0.584814). Saving  the model...\n",
            "Validation loss decreased (0.584814 ===> 0.580861). Saving  the model...\n",
            "Validation loss decreased (0.580861 ===> 0.580828). Saving  the model...\n",
            "Validation loss decreased (0.580828 ===> 0.580448). Saving  the model...\n",
            "Validation loss decreased (0.580448 ===> 0.578309). Saving  the model...\n",
            "Validation loss decreased (0.578309 ===> 0.577417). Saving  the model...\n",
            "Validation loss decreased (0.577417 ===> 0.576201). Saving  the model...\n",
            "Validation loss decreased (0.576201 ===> 0.576167). Saving  the model...\n",
            "Validation loss decreased (0.576167 ===> 0.574178). Saving  the model...\n",
            "Validation loss decreased (0.574178 ===> 0.573483). Saving  the model...\n",
            "Validation loss decreased (0.573483 ===> 0.571801). Saving  the model...\n",
            "Validation loss decreased (0.571801 ===> 0.571304). Saving  the model...\n",
            "Validation loss decreased (0.571304 ===> 0.568225). Saving  the model...\n",
            "Validation loss decreased (0.568225 ===> 0.567063). Saving  the model...\n",
            "Validation loss decreased (0.567063 ===> 0.566608). Saving  the model...\n",
            "Validation loss decreased (0.566608 ===> 0.564920). Saving  the model...\n",
            "Validation loss decreased (0.564920 ===> 0.562279). Saving  the model...\n",
            "Validation loss decreased (0.562279 ===> 0.561216). Saving  the model...\n",
            "\n",
            "Epoch: 201 \tTrain Loss: 0.5636371582953413 \tTrain Accuracy: 0.0\n",
            "Training Ended!\n",
            "Validation loss decreased (0.561216 ===> 0.558181). Saving  the model...\n",
            "Validation loss decreased (0.558181 ===> 0.557431). Saving  the model...\n",
            "Validation loss decreased (0.557431 ===> 0.556349). Saving  the model...\n",
            "Validation loss decreased (0.556349 ===> 0.556041). Saving  the model...\n",
            "Validation loss decreased (0.556041 ===> 0.553241). Saving  the model...\n",
            "Validation loss decreased (0.553241 ===> 0.552806). Saving  the model...\n",
            "Validation loss decreased (0.552806 ===> 0.552433). Saving  the model...\n",
            "Validation loss decreased (0.552433 ===> 0.552130). Saving  the model...\n",
            "Validation loss decreased (0.552130 ===> 0.551275). Saving  the model...\n",
            "Validation loss decreased (0.551275 ===> 0.548799). Saving  the model...\n",
            "Validation loss decreased (0.548799 ===> 0.545404). Saving  the model...\n",
            "Validation loss decreased (0.545404 ===> 0.543487). Saving  the model...\n",
            "Validation loss decreased (0.543487 ===> 0.542902). Saving  the model...\n",
            "Validation loss decreased (0.542902 ===> 0.541567). Saving  the model...\n",
            "Validation loss decreased (0.541567 ===> 0.540743). Saving  the model...\n",
            "Validation loss decreased (0.540743 ===> 0.538780). Saving  the model...\n",
            "Validation loss decreased (0.538780 ===> 0.538226). Saving  the model...\n",
            "Validation loss decreased (0.538226 ===> 0.537990). Saving  the model...\n",
            "Validation loss decreased (0.537990 ===> 0.537963). Saving  the model...\n",
            "Validation loss decreased (0.537963 ===> 0.537857). Saving  the model...\n",
            "Validation loss decreased (0.537857 ===> 0.537709). Saving  the model...\n",
            "Validation loss decreased (0.537709 ===> 0.536596). Saving  the model...\n",
            "Validation loss decreased (0.536596 ===> 0.535376). Saving  the model...\n",
            "Validation loss decreased (0.535376 ===> 0.534288). Saving  the model...\n",
            "Validation loss decreased (0.534288 ===> 0.533855). Saving  the model...\n",
            "Validation loss decreased (0.533855 ===> 0.529349). Saving  the model...\n",
            "Validation loss decreased (0.529349 ===> 0.528995). Saving  the model...\n",
            "Validation loss decreased (0.528995 ===> 0.527528). Saving  the model...\n",
            "Validation loss decreased (0.527528 ===> 0.527343). Saving  the model...\n",
            "Validation loss decreased (0.527343 ===> 0.526980). Saving  the model...\n",
            "Validation loss decreased (0.526980 ===> 0.524933). Saving  the model...\n",
            "Validation loss decreased (0.524933 ===> 0.524270). Saving  the model...\n",
            "Validation loss decreased (0.524270 ===> 0.523002). Saving  the model...\n",
            "Validation loss decreased (0.523002 ===> 0.521008). Saving  the model...\n",
            "Validation loss decreased (0.521008 ===> 0.520442). Saving  the model...\n",
            "Validation loss decreased (0.520442 ===> 0.520370). Saving  the model...\n",
            "Validation loss decreased (0.520370 ===> 0.519045). Saving  the model...\n",
            "Validation loss decreased (0.519045 ===> 0.518590). Saving  the model...\n",
            "Validation loss decreased (0.518590 ===> 0.516176). Saving  the model...\n",
            "Validation loss decreased (0.516176 ===> 0.516075). Saving  the model...\n",
            "Validation loss decreased (0.516075 ===> 0.513764). Saving  the model...\n",
            "Validation loss decreased (0.513764 ===> 0.512946). Saving  the model...\n",
            "Validation loss decreased (0.512946 ===> 0.511384). Saving  the model...\n",
            "\n",
            "Epoch: 401 \tTrain Loss: 0.5130875134890422 \tTrain Accuracy: 0.0\n",
            "Training Ended!\n",
            "Validation loss decreased (0.511384 ===> 0.509035). Saving  the model...\n",
            "Validation loss decreased (0.509035 ===> 0.509025). Saving  the model...\n",
            "Validation loss decreased (0.509025 ===> 0.508943). Saving  the model...\n",
            "Validation loss decreased (0.508943 ===> 0.506035). Saving  the model...\n",
            "Validation loss decreased (0.506035 ===> 0.504652). Saving  the model...\n",
            "Validation loss decreased (0.504652 ===> 0.503589). Saving  the model...\n",
            "Validation loss decreased (0.503589 ===> 0.502457). Saving  the model...\n",
            "Validation loss decreased (0.502457 ===> 0.501334). Saving  the model...\n",
            "Validation loss decreased (0.501334 ===> 0.500620). Saving  the model...\n",
            "Validation loss decreased (0.500620 ===> 0.498996). Saving  the model...\n",
            "Validation loss decreased (0.498996 ===> 0.496093). Saving  the model...\n",
            "Validation loss decreased (0.496093 ===> 0.495672). Saving  the model...\n",
            "Validation loss decreased (0.495672 ===> 0.495333). Saving  the model...\n",
            "Validation loss decreased (0.495333 ===> 0.494272). Saving  the model...\n",
            "Validation loss decreased (0.494272 ===> 0.491916). Saving  the model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNjSGtvl0vtw"
      },
      "source": [
        "# Prediction\r\n",
        "\r\n",
        "X_test = df_test.iloc[:, 1:].values\r\n",
        "X_test_var = Variable(torch.FloatTensor(X_test), requires_grad=False) # requires_grad=True: autograd가 텐서의 추적 기록에 남지 않게 함\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    test_result = model(X_test_var)\r\n",
        "values, labels = torch.max(test_result, 1)\r\n",
        "survived = labels.data.numpy()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jTJCvB02iVm"
      },
      "source": [
        "# Submission\r\n",
        "\r\n",
        "submission = pd.DataFrame({'PassengerId': df_sub['PassengerId'], 'Survived': survived})\r\n",
        "submission.to_csv('/content/drive/MyDrive/Colab Notebooks/Titanic/titanic/submission_predict.csv', index=False)"
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}